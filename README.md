# rag-llm-showcase

Agentic RAG Architecture Diagram
RAG Pipeline Comparison
ðŸ”· Regular RAG Workflow
Document Upload â†’ text chunking / Embedding â†’ vector Database (pinecone)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Query Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Query
â†“
Embed Query
â†“
Vector Search (Pinecone Top-K retrieval)
â†“
LLM Prompt Engineering
â†“
LLM Generates Answer



ðŸ”· Agentic RAG Workflow
Document Upload â†’ Text Chunking (recursive with overlap) â†’ Metadata Extraction (Summaries, Entities, Keywords) â†’ Embeddings (OpenAI text-embedding-3-small) â†’ Vector Storage (Pinecone with Metadata)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Query Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Query
â†“
LLM Query Decomposition
â†“
Sub-Queries
â†“
For Each Sub-Query:
â†’ Embed Sub-Query
â†’ Vector Search (Pinecone Retrieval)
â†’ Context â†’ LLM Sub-Answer
â†“
Sub-Answers Collected
â†“
LLM Final Synthesis
â†“
Final Answer